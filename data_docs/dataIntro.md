# Data æ¨¡å—ä»‹ç»

## ğŸ¯ æ¦‚è¿°

`data` æ˜¯ RedFire é‡åŒ–äº¤æ˜“å¹³å°çš„æ•°æ®ç®¡ç†æ¨¡å—ï¼Œè´Ÿè´£å­˜å‚¨å’Œç®¡ç†å„ç§ç±»å‹çš„äº¤æ˜“æ•°æ®ã€å¸‚åœºæ•°æ®ã€ç ”ç©¶æ•°æ®ç­‰ã€‚è¯¥æ¨¡å—æä¾›æ•°æ®å­˜å‚¨ã€æ•°æ®è®¿é—®ã€æ•°æ®åŒæ­¥ã€æ•°æ®å¤‡ä»½ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸ºé‡åŒ–äº¤æ˜“ç­–ç•¥æä¾›å¯é çš„æ•°æ®æ”¯æŒã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
data/
â”œâ”€â”€ factor-data/             # ğŸ“Š å› å­æ•°æ®
â”œâ”€â”€ research-data/           # ğŸ”¬ ç ”ç©¶æ•°æ®
â”œâ”€â”€ market-data/             # ğŸ“ˆ å¸‚åœºæ•°æ®
â”œâ”€â”€ market_data/             # ğŸ“Š å¸‚åœºæ•°æ®(å¤‡ç”¨)
â”œâ”€â”€ databases/               # ğŸ—„ï¸ æ•°æ®åº“æ–‡ä»¶
â”œâ”€â”€ logs/                    # ğŸ“ æ•°æ®æ—¥å¿—
â””â”€â”€ init-db.sql              # ğŸ—ƒï¸ æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
```

## ğŸ“Š æ•°æ®åˆ†ç±»è¯¦è§£

### 1. **å› å­æ•°æ®** (`factor-data/`)

**ä½œç”¨**: å­˜å‚¨é‡åŒ–å› å­æ•°æ®ï¼Œç”¨äºå› å­åˆ†æå’Œç­–ç•¥å¼€å‘

**æ•°æ®ç±»å‹**:
- **æŠ€æœ¯å› å­**: ç§»åŠ¨å¹³å‡ã€RSIã€MACDç­‰
- **åŸºæœ¬é¢å› å­**: PEã€PBã€ROEç­‰
- **æƒ…ç»ªå› å­**: æˆäº¤é‡ã€æ¢æ‰‹ç‡ç­‰
- **å®è§‚å› å­**: åˆ©ç‡ã€æ±‡ç‡ã€GDPç­‰
- **è‡ªå®šä¹‰å› å­**: ç”¨æˆ·å®šä¹‰çš„å› å­

**æ•°æ®æ ¼å¼**:
```python
# å› å­æ•°æ®ç»“æ„
{
    "factor_name": "MA_20",
    "symbol": "000001.SZ",
    "date": "2024-01-01",
    "value": 15.23,
    "source": "calculated",
    "update_time": "2024-01-01 09:30:00"
}
```

**å­˜å‚¨æ–¹å¼**:
- Parquetæ–‡ä»¶ (é«˜æ•ˆå‹ç¼©)
- CSVæ–‡ä»¶ (æ˜“è¯»æ˜“å†™)
- HDF5æ–‡ä»¶ (å¤§æ–‡ä»¶æ”¯æŒ)
- æ•°æ®åº“å­˜å‚¨ (å®æ—¶æŸ¥è¯¢)

### 2. **ç ”ç©¶æ•°æ®** (`research-data/`)

**ä½œç”¨**: å­˜å‚¨ç ”ç©¶åˆ†æç›¸å…³çš„æ•°æ®

**æ•°æ®ç±»å‹**:
- **å›æµ‹ç»“æœ**: ç­–ç•¥å›æµ‹æ•°æ®
- **ç ”ç©¶æŠ¥å‘Š**: åˆ†ææŠ¥å‘Šæ•°æ®
- **æ¨¡å‹æ•°æ®**: æœºå™¨å­¦ä¹ æ¨¡å‹æ•°æ®
- **å®éªŒæ•°æ®**: ç ”ç©¶å®éªŒæ•°æ®
- **åŸºå‡†æ•°æ®**: åŸºå‡†æŒ‡æ•°æ•°æ®

**æ•°æ®ç»„ç»‡**:
```
research-data/
â”œâ”€â”€ backtest/                # å›æµ‹ç»“æœ
â”‚   â”œâ”€â”€ strategy_001/
â”‚   â”œâ”€â”€ strategy_002/
â”‚   â””â”€â”€ comparison/
â”œâ”€â”€ reports/                 # ç ”ç©¶æŠ¥å‘Š
â”‚   â”œâ”€â”€ monthly/
â”‚   â”œâ”€â”€ quarterly/
â”‚   â””â”€â”€ annual/
â”œâ”€â”€ models/                  # æ¨¡å‹æ•°æ®
â”‚   â”œâ”€â”€ ml_models/
â”‚   â”œâ”€â”€ deep_learning/
â”‚   â””â”€â”€ ensemble/
â””â”€â”€ experiments/             # å®éªŒæ•°æ®
    â”œâ”€â”€ factor_analysis/
    â”œâ”€â”€ risk_analysis/
    â””â”€â”€ performance_analysis/
```

### 3. **å¸‚åœºæ•°æ®** (`market-data/`, `market_data/`)

**ä½œç”¨**: å­˜å‚¨å®æ—¶å’Œå†å²å¸‚åœºæ•°æ®

**æ•°æ®ç±»å‹**:
- **è¡Œæƒ…æ•°æ®**: å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€æœ€é«˜ä»·ã€æœ€ä½ä»·ã€æˆäº¤é‡
- **åˆ†æ—¶æ•°æ®**: åˆ†é’Ÿçº§ã€ç§’çº§è¡Œæƒ…æ•°æ®
- **Tickæ•°æ®**: é€ç¬”äº¤æ˜“æ•°æ®
- **Level2æ•°æ®**: æ·±åº¦è¡Œæƒ…æ•°æ®
- **è´¢åŠ¡æ•°æ®**: è´¢åŠ¡æŠ¥è¡¨æ•°æ®
- **å…¬å‘Šæ•°æ®**: å…¬å¸å…¬å‘Šä¿¡æ¯

**æ•°æ®æ ¼å¼**:
```python
# è¡Œæƒ…æ•°æ®ç»“æ„
{
    "symbol": "000001.SZ",
    "date": "2024-01-01",
    "open": 15.20,
    "high": 15.50,
    "low": 15.10,
    "close": 15.35,
    "volume": 1000000,
    "amount": 15350000,
    "turnover_rate": 0.85
}

# Tickæ•°æ®ç»“æ„
{
    "symbol": "000001.SZ",
    "timestamp": "2024-01-01 09:30:00.123",
    "price": 15.23,
    "volume": 100,
    "direction": "buy",
    "order_id": "123456789"
}
```

### 4. **æ•°æ®åº“æ–‡ä»¶** (`databases/`)

**ä½œç”¨**: å­˜å‚¨å„ç§æ•°æ®åº“æ–‡ä»¶

**æ•°æ®åº“ç±»å‹**:
- **SQLite**: è½»é‡çº§æ•°æ®åº“
- **PostgreSQL**: å…³ç³»å‹æ•°æ®åº“
- **MongoDB**: æ–‡æ¡£å‹æ•°æ®åº“
- **InfluxDB**: æ—¶åºæ•°æ®åº“
- **Redis**: å†…å­˜æ•°æ®åº“

**æ•°æ®ç»„ç»‡**:
```
databases/
â”œâ”€â”€ sqlite/                  # SQLiteæ•°æ®åº“
â”‚   â”œâ”€â”€ redfire.db
â”‚   â”œâ”€â”€ vnpy.db
â”‚   â””â”€â”€ backup/
â”œâ”€â”€ postgresql/              # PostgreSQLæ•°æ®
â”‚   â”œâ”€â”€ dumps/
â”‚   â””â”€â”€ migrations/
â”œâ”€â”€ mongodb/                 # MongoDBæ•°æ®
â”‚   â”œâ”€â”€ collections/
â”‚   â””â”€â”€ indexes/
â”œâ”€â”€ influxdb/                # InfluxDBæ•°æ®
â”‚   â”œâ”€â”€ measurements/
â”‚   â””â”€â”€ retention_policies/
â””â”€â”€ redis/                   # Redisæ•°æ®
    â”œâ”€â”€ rdb/
    â””â”€â”€ aof/
```

### 5. **æ•°æ®æ—¥å¿—** (`logs/`)

**ä½œç”¨**: è®°å½•æ•°æ®æ“ä½œå’Œå¤„ç†æ—¥å¿—

**æ—¥å¿—ç±»å‹**:
- **æ•°æ®åŒæ­¥æ—¥å¿—**: æ•°æ®åŒæ­¥çŠ¶æ€
- **æ•°æ®å¤„ç†æ—¥å¿—**: æ•°æ®å¤„ç†è¿‡ç¨‹
- **é”™è¯¯æ—¥å¿—**: æ•°æ®é”™è¯¯ä¿¡æ¯
- **æ€§èƒ½æ—¥å¿—**: æ•°æ®è®¿é—®æ€§èƒ½
- **å®¡è®¡æ—¥å¿—**: æ•°æ®è®¿é—®å®¡è®¡

**æ—¥å¿—æ ¼å¼**:
```python
# æ—¥å¿—è®°å½•ç»“æ„
{
    "timestamp": "2024-01-01 10:00:00",
    "level": "INFO",
    "module": "data_sync",
    "message": "æ•°æ®åŒæ­¥å®Œæˆ",
    "details": {
        "symbol": "000001.SZ",
        "records": 1000,
        "duration": 5.2
    }
}
```

## ğŸ”„ æ•°æ®ç®¡ç†åŠŸèƒ½

### **1. æ•°æ®åŒæ­¥**

```python
class DataSynchronizer:
    def __init__(self, source_config, target_config):
        self.source = DataSource(source_config)
        self.target = DataTarget(target_config)
    
    async def sync_market_data(self, symbols, start_date, end_date):
        """åŒæ­¥å¸‚åœºæ•°æ®"""
        for symbol in symbols:
            data = await self.source.get_market_data(symbol, start_date, end_date)
            await self.target.save_market_data(data)
    
    async def sync_factor_data(self, factors, symbols, date_range):
        """åŒæ­¥å› å­æ•°æ®"""
        for factor in factors:
            data = await self.source.get_factor_data(factor, symbols, date_range)
            await self.target.save_factor_data(data)
```

### **2. æ•°æ®éªŒè¯**

```python
class DataValidator:
    def validate_market_data(self, data):
        """éªŒè¯å¸‚åœºæ•°æ®"""
        errors = []
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']
        for field in required_fields:
            if field not in data.columns:
                errors.append(f"Missing required field: {field}")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if data['volume'].min() < 0:
            errors.append("Volume cannot be negative")
        
        # æ£€æŸ¥ä»·æ ¼é€»è¾‘
        if (data['high'] < data['low']).any():
            errors.append("High price cannot be lower than low price")
        
        return errors
```

### **3. æ•°æ®æ¸…æ´—**

```python
class DataCleaner:
    def clean_market_data(self, data):
        """æ¸…æ´—å¸‚åœºæ•°æ®"""
        # å»é™¤é‡å¤æ•°æ®
        data = data.drop_duplicates()
        
        # å¤„ç†ç¼ºå¤±å€¼
        data = data.fillna(method='ffill')
        
        # å¼‚å¸¸å€¼å¤„ç†
        data = self.remove_outliers(data)
        
        # æ•°æ®æ ‡å‡†åŒ–
        data = self.normalize_data(data)
        
        return data
    
    def remove_outliers(self, data, method='iqr'):
        """å»é™¤å¼‚å¸¸å€¼"""
        if method == 'iqr':
            Q1 = data['close'].quantile(0.25)
            Q3 = data['close'].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            return data[(data['close'] >= lower_bound) & (data['close'] <= upper_bound)]
```

### **4. æ•°æ®å¤‡ä»½**

```python
class DataBackup:
    def __init__(self, backup_config):
        self.backup_path = backup_config['path']
        self.retention_days = backup_config['retention_days']
    
    async def backup_database(self, database_name):
        """å¤‡ä»½æ•°æ®åº“"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = f"{self.backup_path}/{database_name}_{timestamp}.sql"
        
        # æ‰§è¡Œå¤‡ä»½
        await self.execute_backup(database_name, backup_file)
        
        # æ¸…ç†æ—§å¤‡ä»½
        await self.cleanup_old_backups()
    
    async def restore_database(self, database_name, backup_file):
        """æ¢å¤æ•°æ®åº“"""
        await self.execute_restore(database_name, backup_file)
```

## ğŸ“ˆ æ•°æ®è®¿é—®æ¥å£

### **1. å¸‚åœºæ•°æ®è®¿é—®**

```python
class MarketDataAccess:
    def __init__(self, data_source):
        self.data_source = data_source
    
    async def get_daily_data(self, symbol, start_date, end_date):
        """è·å–æ—¥çº¿æ•°æ®"""
        return await self.data_source.query(
            "SELECT * FROM market_data WHERE symbol = %s AND date BETWEEN %s AND %s",
            (symbol, start_date, end_date)
        )
    
    async def get_realtime_data(self, symbols):
        """è·å–å®æ—¶æ•°æ®"""
        return await self.data_source.query(
            "SELECT * FROM realtime_data WHERE symbol IN %s",
            (tuple(symbols),)
        )
    
    async def get_tick_data(self, symbol, start_time, end_time):
        """è·å–Tickæ•°æ®"""
        return await self.data_source.query(
            "SELECT * FROM tick_data WHERE symbol = %s AND timestamp BETWEEN %s AND %s",
            (symbol, start_time, end_time)
        )
```

### **2. å› å­æ•°æ®è®¿é—®**

```python
class FactorDataAccess:
    def __init__(self, data_source):
        self.data_source = data_source
    
    async def get_factor_data(self, factor_name, symbols, date_range):
        """è·å–å› å­æ•°æ®"""
        return await self.data_source.query(
            "SELECT * FROM factor_data WHERE factor_name = %s AND symbol IN %s AND date BETWEEN %s AND %s",
            (factor_name, tuple(symbols), date_range[0], date_range[1])
        )
    
    async def calculate_factor(self, factor_name, symbols, start_date, end_date):
        """è®¡ç®—å› å­æ•°æ®"""
        # è·å–åŸºç¡€æ•°æ®
        market_data = await self.get_market_data(symbols, start_date, end_date)
        
        # è®¡ç®—å› å­
        factor_data = self.calculate_factor_value(factor_name, market_data)
        
        # ä¿å­˜å› å­æ•°æ®
        await self.save_factor_data(factor_data)
        
        return factor_data
```

### **3. ç ”ç©¶æ•°æ®è®¿é—®**

```python
class ResearchDataAccess:
    def __init__(self, data_source):
        self.data_source = data_source
    
    async def save_backtest_result(self, strategy_name, result_data):
        """ä¿å­˜å›æµ‹ç»“æœ"""
        return await self.data_source.insert(
            "backtest_results",
            {
                "strategy_name": strategy_name,
                "result_data": json.dumps(result_data),
                "created_at": datetime.now()
            }
        )
    
    async def get_backtest_results(self, strategy_name):
        """è·å–å›æµ‹ç»“æœ"""
        results = await self.data_source.query(
            "SELECT * FROM backtest_results WHERE strategy_name = %s ORDER BY created_at DESC",
            (strategy_name,)
        )
        return [json.loads(result['result_data']) for result in results]
```

## ğŸ”§ æ•°æ®å·¥å…·

### **1. æ•°æ®ä¸‹è½½å·¥å…·**

```python
class DataDownloader:
    def __init__(self, api_config):
        self.api = DataAPI(api_config)
    
    async def download_market_data(self, symbols, start_date, end_date):
        """ä¸‹è½½å¸‚åœºæ•°æ®"""
        tasks = []
        for symbol in symbols:
            task = self.download_symbol_data(symbol, start_date, end_date)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        return results
    
    async def download_symbol_data(self, symbol, start_date, end_date):
        """ä¸‹è½½å•ä¸ªè‚¡ç¥¨æ•°æ®"""
        data = await self.api.get_market_data(symbol, start_date, end_date)
        await self.save_data(symbol, data)
        return data
```

### **2. æ•°æ®è½¬æ¢å·¥å…·**

```python
class DataConverter:
    def convert_to_parquet(self, csv_file, parquet_file):
        """CSVè½¬Parquet"""
        df = pd.read_csv(csv_file)
        df.to_parquet(parquet_file, index=False)
    
    def convert_to_hdf5(self, data, hdf5_file, key):
        """æ•°æ®è½¬HDF5"""
        df = pd.DataFrame(data)
        df.to_hdf(hdf5_file, key=key, mode='w')
    
    def convert_timezone(self, data, from_tz, to_tz):
        """æ—¶åŒºè½¬æ¢"""
        data['timestamp'] = pd.to_datetime(data['timestamp'])
        data['timestamp'] = data['timestamp'].dt.tz_localize(from_tz).dt.tz_convert(to_tz)
        return data
```

### **3. æ•°æ®è´¨é‡æ£€æŸ¥**

```python
class DataQualityChecker:
    def check_data_completeness(self, data, expected_fields):
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        missing_fields = set(expected_fields) - set(data.columns)
        if missing_fields:
            return False, f"Missing fields: {missing_fields}"
        return True, "Data complete"
    
    def check_data_consistency(self, data):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        errors = []
        
        # æ£€æŸ¥æ•°æ®ç±»å‹
        for column, expected_type in data.dtypes.items():
            if not pd.api.types.is_dtype_equal(data[column].dtype, expected_type):
                errors.append(f"Type mismatch for column {column}")
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if 'price' in data.columns:
            if (data['price'] <= 0).any():
                errors.append("Price must be positive")
        
        return len(errors) == 0, errors
```

## ğŸ“Š æ•°æ®ç›‘æ§

### **1. æ•°æ®è´¨é‡ç›‘æ§**

```python
class DataQualityMonitor:
    def __init__(self):
        self.quality_metrics = {}
    
    async def monitor_data_quality(self, data_source):
        """ç›‘æ§æ•°æ®è´¨é‡"""
        metrics = {
            "completeness": await self.check_completeness(data_source),
            "accuracy": await self.check_accuracy(data_source),
            "timeliness": await self.check_timeliness(data_source),
            "consistency": await self.check_consistency(data_source)
        }
        
        self.quality_metrics[data_source] = metrics
        return metrics
    
    async def generate_quality_report(self):
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        report = {
            "timestamp": datetime.now(),
            "overall_score": self.calculate_overall_score(),
            "details": self.quality_metrics
        }
        return report
```

### **2. æ•°æ®æ€§èƒ½ç›‘æ§**

```python
class DataPerformanceMonitor:
    def __init__(self):
        self.performance_metrics = {}
    
    async def monitor_query_performance(self, query_name, query_func):
        """ç›‘æ§æŸ¥è¯¢æ€§èƒ½"""
        start_time = time.time()
        result = await query_func()
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        self.performance_metrics[query_name] = {
            "execution_time": execution_time,
            "timestamp": datetime.now(),
            "result_size": len(result) if result else 0
        }
        
        return result
```

## ğŸš€ æ•°æ®ä¼˜åŒ–

### **1. æ•°æ®å‹ç¼©**

```python
class DataCompressor:
    def compress_parquet(self, input_file, output_file):
        """å‹ç¼©Parquetæ–‡ä»¶"""
        df = pd.read_parquet(input_file)
        df.to_parquet(output_file, compression='gzip')
    
    def compress_csv(self, input_file, output_file):
        """å‹ç¼©CSVæ–‡ä»¶"""
        df = pd.read_csv(input_file)
        df.to_csv(output_file, compression='gzip', index=False)
```

### **2. æ•°æ®ç´¢å¼•**

```python
class DataIndexer:
    def create_index(self, table_name, columns):
        """åˆ›å»ºæ•°æ®åº“ç´¢å¼•"""
        for column in columns:
            index_name = f"idx_{table_name}_{column}"
            query = f"CREATE INDEX {index_name} ON {table_name} ({column})"
            await self.execute_query(query)
    
    def optimize_queries(self, query_patterns):
        """ä¼˜åŒ–æŸ¥è¯¢æ¨¡å¼"""
        # åˆ†ææŸ¥è¯¢æ¨¡å¼å¹¶åˆ›å»ºåˆé€‚çš„ç´¢å¼•
        pass
```

---

**æ€»ç»“**: Dataæ¨¡å—æ˜¯RedFireå¹³å°çš„æ•°æ®ç®¡ç†ä¸­å¿ƒï¼Œæä¾›å®Œæ•´çš„æ•°æ®å­˜å‚¨ã€ç®¡ç†ã€è®¿é—®å’Œç›‘æ§åŠŸèƒ½ã€‚é€šè¿‡è§„èŒƒåŒ–çš„æ•°æ®ç»„ç»‡ã€å®Œå–„çš„æ•°æ®è´¨é‡æ§åˆ¶å’Œé«˜æ•ˆçš„è®¿é—®æ¥å£ï¼Œä¸ºé‡åŒ–äº¤æ˜“ç­–ç•¥æä¾›å¯é çš„æ•°æ®æ”¯æŒã€‚
