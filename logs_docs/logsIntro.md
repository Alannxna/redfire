# Logs æ¨¡å—ä»‹ç»

## ğŸ¯ æ¦‚è¿°

`logs` æ˜¯ RedFire é‡åŒ–äº¤æ˜“å¹³å°çš„æ—¥å¿—ç®¡ç†æ¨¡å—ï¼Œè´Ÿè´£æ”¶é›†ã€å­˜å‚¨å’Œç®¡ç†ç³»ç»Ÿè¿è¡Œè¿‡ç¨‹ä¸­çš„å„ç§æ—¥å¿—ä¿¡æ¯ã€‚è¯¥æ¨¡å—æä¾›ç»“æ„åŒ–çš„æ—¥å¿—è®°å½•ã€æ—¥å¿—åˆ†æå’Œç›‘æ§åŠŸèƒ½ï¼Œç¡®ä¿ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§å’Œé—®é¢˜æ’æŸ¥èƒ½åŠ›ã€‚

## ğŸ“ ç›®å½•ç»“æ„

```
logs/
â”œâ”€â”€ access/                   # ğŸŒ è®¿é—®æ—¥å¿—
â”œâ”€â”€ audit/                    # ğŸ” å®¡è®¡æ—¥å¿—
â”œâ”€â”€ error/                    # âŒ é”™è¯¯æ—¥å¿—
â””â”€â”€ application/              # ğŸ“ åº”ç”¨æ—¥å¿—
```

## ğŸ“ æ—¥å¿—åˆ†ç±»è¯¦è§£

### 1. **è®¿é—®æ—¥å¿—** (`access/`)

**ä½œç”¨**: è®°å½•ç”¨æˆ·è®¿é—®å’ŒAPIè°ƒç”¨ä¿¡æ¯

**è®°å½•å†…å®¹**:
- HTTPè¯·æ±‚æ—¥å¿—
- APIè°ƒç”¨è®°å½•
- ç”¨æˆ·è®¿é—®ç»Ÿè®¡
- æ€§èƒ½æŒ‡æ ‡

**æ—¥å¿—æ ¼å¼**:
```json
{
  "timestamp": "2024-01-01T10:00:00Z",
  "level": "INFO",
  "type": "access",
  "user_id": "user123",
  "ip_address": "192.168.1.100",
  "method": "POST",
  "path": "/api/orders",
  "status_code": 200,
  "response_time": 150,
  "user_agent": "Mozilla/5.0...",
  "request_size": 1024,
  "response_size": 512
}
```

### 2. **å®¡è®¡æ—¥å¿—** (`audit/`)

**ä½œç”¨**: è®°å½•ç³»ç»Ÿæ“ä½œå’Œå˜æ›´å®¡è®¡ä¿¡æ¯

**è®°å½•å†…å®¹**:
- ç”¨æˆ·æ“ä½œè®°å½•
- ç³»ç»Ÿé…ç½®å˜æ›´
- æƒé™å˜æ›´
- æ•°æ®ä¿®æ”¹è®°å½•

**æ—¥å¿—æ ¼å¼**:
```json
{
  "timestamp": "2024-01-01T10:00:00Z",
  "level": "INFO",
  "type": "audit",
  "user_id": "user123",
  "action": "CREATE_ORDER",
  "resource": "orders",
  "resource_id": "order456",
  "details": {
    "symbol": "AAPL",
    "quantity": 100,
    "price": 150.00
  },
  "ip_address": "192.168.1.100",
  "session_id": "session789"
}
```

### 3. **é”™è¯¯æ—¥å¿—** (`error/`)

**ä½œç”¨**: è®°å½•ç³»ç»Ÿé”™è¯¯å’Œå¼‚å¸¸ä¿¡æ¯

**è®°å½•å†…å®¹**:
- ç³»ç»Ÿå¼‚å¸¸
- ä¸šåŠ¡é”™è¯¯
- è¿æ¥å¤±è´¥
- æ€§èƒ½é—®é¢˜

**æ—¥å¿—æ ¼å¼**:
```json
{
  "timestamp": "2024-01-01T10:00:00Z",
  "level": "ERROR",
  "type": "error",
  "error_code": "DB_CONNECTION_FAILED",
  "error_message": "Database connection timeout",
  "stack_trace": "Traceback (most recent call last)...",
  "context": {
    "user_id": "user123",
    "request_id": "req456",
    "module": "database"
  },
  "severity": "HIGH"
}
```

### 4. **åº”ç”¨æ—¥å¿—** (`application/`)

**ä½œç”¨**: è®°å½•åº”ç”¨ç¨‹åºè¿è¡Œä¿¡æ¯

**è®°å½•å†…å®¹**:
- åº”ç”¨å¯åŠ¨/å…³é—­
- ä¸šåŠ¡é€»è¾‘æ‰§è¡Œ
- æ€§èƒ½ç›‘æ§
- è°ƒè¯•ä¿¡æ¯

**æ—¥å¿—æ ¼å¼**:
```json
{
  "timestamp": "2024-01-01T10:00:00Z",
  "level": "INFO",
  "type": "application",
  "module": "trading_engine",
  "function": "process_order",
  "message": "Order processed successfully",
  "data": {
    "order_id": "order123",
    "processing_time": 50
  }
}
```

## ğŸ”§ æ—¥å¿—ç®¡ç†åŠŸèƒ½

### **1. æ—¥å¿—æ”¶é›†**

```python
class LogCollector:
    def __init__(self, config):
        self.config = config
        self.loggers = {}
    
    def setup_logger(self, name, log_type):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(name)
        logger.setLevel(logging.INFO)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(
            f"logs/{log_type}/{name}.log"
        )
        file_handler.setFormatter(self.get_formatter())
        logger.addHandler(file_handler)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(self.get_formatter())
        logger.addHandler(console_handler)
        
        self.loggers[name] = logger
        return logger
    
    def get_formatter(self):
        """è·å–æ—¥å¿—æ ¼å¼å™¨"""
        return logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def log_access(self, request, response, response_time):
        """è®°å½•è®¿é—®æ—¥å¿—"""
        logger = self.loggers.get('access')
        if logger:
            log_data = {
                'timestamp': datetime.now().isoformat(),
                'user_id': request.user.id if hasattr(request, 'user') else None,
                'ip_address': request.client.host,
                'method': request.method,
                'path': request.url.path,
                'status_code': response.status_code,
                'response_time': response_time,
                'user_agent': request.headers.get('user-agent')
            }
            logger.info(json.dumps(log_data))
    
    def log_audit(self, user_id, action, resource, details):
        """è®°å½•å®¡è®¡æ—¥å¿—"""
        logger = self.loggers.get('audit')
        if logger:
            log_data = {
                'timestamp': datetime.now().isoformat(),
                'user_id': user_id,
                'action': action,
                'resource': resource,
                'details': details
            }
            logger.info(json.dumps(log_data))
    
    def log_error(self, error, context=None):
        """è®°å½•é”™è¯¯æ—¥å¿—"""
        logger = self.loggers.get('error')
        if logger:
            log_data = {
                'timestamp': datetime.now().isoformat(),
                'error_code': getattr(error, 'code', 'UNKNOWN'),
                'error_message': str(error),
                'stack_trace': traceback.format_exc(),
                'context': context or {}
            }
            logger.error(json.dumps(log_data))
```

### **2. æ—¥å¿—åˆ†æ**

```python
class LogAnalyzer:
    def __init__(self, logs_path):
        self.logs_path = logs_path
    
    def analyze_access_patterns(self, start_time, end_time):
        """åˆ†æè®¿é—®æ¨¡å¼"""
        access_logs = self.load_logs('access', start_time, end_time)
        
        analysis = {
            'total_requests': len(access_logs),
            'unique_users': len(set(log['user_id'] for log in access_logs if log['user_id'])),
            'top_endpoints': self.get_top_endpoints(access_logs),
            'response_time_stats': self.get_response_time_stats(access_logs),
            'error_rate': self.calculate_error_rate(access_logs)
        }
        
        return analysis
    
    def analyze_error_patterns(self, start_time, end_time):
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        error_logs = self.load_logs('error', start_time, end_time)
        
        analysis = {
            'total_errors': len(error_logs),
            'error_types': self.group_by_error_type(error_logs),
            'most_frequent_errors': self.get_most_frequent_errors(error_logs),
            'error_trends': self.get_error_trends(error_logs)
        }
        
        return analysis
    
    def analyze_user_activity(self, user_id, start_time, end_time):
        """åˆ†æç”¨æˆ·æ´»åŠ¨"""
        access_logs = self.load_logs('access', start_time, end_time)
        user_logs = [log for log in access_logs if log.get('user_id') == user_id]
        
        analysis = {
            'total_actions': len(user_logs),
            'action_types': self.group_by_action_type(user_logs),
            'peak_activity_hours': self.get_peak_activity_hours(user_logs),
            'frequently_accessed_resources': self.get_frequent_resources(user_logs)
        }
        
        return analysis
```

### **3. æ—¥å¿—ç›‘æ§**

```python
class LogMonitor:
    def __init__(self, alert_config):
        self.alert_config = alert_config
        self.alert_handlers = []
    
    def add_alert_handler(self, handler):
        """æ·»åŠ å‘Šè­¦å¤„ç†å™¨"""
        self.alert_handlers.append(handler)
    
    def monitor_error_rate(self, threshold=0.05):
        """ç›‘æ§é”™è¯¯ç‡"""
        current_error_rate = self.calculate_current_error_rate()
        
        if current_error_rate > threshold:
            alert = {
                'type': 'ERROR_RATE_HIGH',
                'message': f'Error rate {current_error_rate:.2%} exceeds threshold {threshold:.2%}',
                'severity': 'HIGH',
                'timestamp': datetime.now()
            }
            self.send_alert(alert)
    
    def monitor_response_time(self, threshold=1000):
        """ç›‘æ§å“åº”æ—¶é—´"""
        current_avg_response_time = self.calculate_avg_response_time()
        
        if current_avg_response_time > threshold:
            alert = {
                'type': 'RESPONSE_TIME_HIGH',
                'message': f'Average response time {current_avg_response_time}ms exceeds threshold {threshold}ms',
                'severity': 'MEDIUM',
                'timestamp': datetime.now()
            }
            self.send_alert(alert)
    
    def monitor_unusual_activity(self):
        """ç›‘æ§å¼‚å¸¸æ´»åŠ¨"""
        unusual_patterns = self.detect_unusual_patterns()
        
        for pattern in unusual_patterns:
            alert = {
                'type': 'UNUSUAL_ACTIVITY',
                'message': f'Detected unusual activity: {pattern["description"]}',
                'severity': 'LOW',
                'timestamp': datetime.now(),
                'details': pattern
            }
            self.send_alert(alert)
    
    def send_alert(self, alert):
        """å‘é€å‘Šè­¦"""
        for handler in self.alert_handlers:
            handler.handle_alert(alert)
```

## ğŸ“Š æ—¥å¿—å­˜å‚¨å’Œæ£€ç´¢

### **1. æ—¥å¿—å­˜å‚¨**

```python
class LogStorage:
    def __init__(self, storage_config):
        self.storage_config = storage_config
        self.storage_backend = self.create_storage_backend()
    
    def create_storage_backend(self):
        """åˆ›å»ºå­˜å‚¨åç«¯"""
        if self.storage_config['type'] == 'elasticsearch':
            return ElasticsearchStorage(self.storage_config)
        elif self.storage_config['type'] == 'file':
            return FileStorage(self.storage_config)
        else:
            raise ValueError(f"Unsupported storage type: {self.storage_config['type']}")
    
    def store_log(self, log_data):
        """å­˜å‚¨æ—¥å¿—"""
        return self.storage_backend.store(log_data)
    
    def search_logs(self, query, start_time=None, end_time=None, limit=100):
        """æœç´¢æ—¥å¿—"""
        return self.storage_backend.search(query, start_time, end_time, limit)
    
    def get_log_statistics(self, start_time, end_time):
        """è·å–æ—¥å¿—ç»Ÿè®¡"""
        return self.storage_backend.get_statistics(start_time, end_time)
```

### **2. æ—¥å¿—æ£€ç´¢**

```python
class LogRetriever:
    def __init__(self, storage):
        self.storage = storage
    
    def search_by_user(self, user_id, start_time=None, end_time=None):
        """æŒ‰ç”¨æˆ·æœç´¢æ—¥å¿—"""
        query = {'user_id': user_id}
        return self.storage.search_logs(query, start_time, end_time)
    
    def search_by_error_type(self, error_type, start_time=None, end_time=None):
        """æŒ‰é”™è¯¯ç±»å‹æœç´¢æ—¥å¿—"""
        query = {'error_code': error_type}
        return self.storage.search_logs(query, start_time, end_time)
    
    def search_by_time_range(self, start_time, end_time):
        """æŒ‰æ—¶é—´èŒƒå›´æœç´¢æ—¥å¿—"""
        return self.storage.search_logs({}, start_time, end_time)
    
    def search_by_keyword(self, keyword, start_time=None, end_time=None):
        """æŒ‰å…³é”®è¯æœç´¢æ—¥å¿—"""
        query = {'message': {'$regex': keyword, '$options': 'i'}}
        return self.storage.search_logs(query, start_time, end_time)
```

## ğŸ” æ—¥å¿—å¯è§†åŒ–

### **1. å®æ—¶ç›‘æ§é¢æ¿**

```python
class LogDashboard:
    def __init__(self, log_analyzer):
        self.log_analyzer = log_analyzer
    
    def get_real_time_metrics(self):
        """è·å–å®æ—¶æŒ‡æ ‡"""
        now = datetime.now()
        start_time = now - timedelta(hours=1)
        
        return {
            'requests_per_minute': self.calculate_requests_per_minute(start_time, now),
            'error_rate': self.calculate_error_rate(start_time, now),
            'average_response_time': self.calculate_avg_response_time(start_time, now),
            'active_users': self.calculate_active_users(start_time, now)
        }
    
    def get_trend_chart_data(self, metric, hours=24):
        """è·å–è¶‹åŠ¿å›¾è¡¨æ•°æ®"""
        now = datetime.now()
        start_time = now - timedelta(hours=hours)
        
        data_points = []
        for i in range(hours):
            point_time = start_time + timedelta(hours=i)
            value = self.get_metric_value(metric, point_time)
            data_points.append({
                'timestamp': point_time.isoformat(),
                'value': value
            })
        
        return data_points
```

### **2. æ—¥å¿—æŠ¥è¡¨**

```python
class LogReporter:
    def __init__(self, log_analyzer):
        self.log_analyzer = log_analyzer
    
    def generate_daily_report(self, date):
        """ç”Ÿæˆæ—¥æŠ¥"""
        start_time = date.replace(hour=0, minute=0, second=0, microsecond=0)
        end_time = start_time + timedelta(days=1)
        
        report = {
            'date': date.date().isoformat(),
            'summary': self.get_daily_summary(start_time, end_time),
            'access_analysis': self.log_analyzer.analyze_access_patterns(start_time, end_time),
            'error_analysis': self.log_analyzer.analyze_error_patterns(start_time, end_time),
            'top_users': self.get_top_users(start_time, end_time),
            'performance_metrics': self.get_performance_metrics(start_time, end_time)
        }
        
        return report
    
    def generate_weekly_report(self, week_start):
        """ç”Ÿæˆå‘¨æŠ¥"""
        end_time = week_start + timedelta(weeks=1)
        
        report = {
            'week_start': week_start.date().isoformat(),
            'summary': self.get_weekly_summary(week_start, end_time),
            'trends': self.get_weekly_trends(week_start, end_time),
            'comparison': self.compare_with_previous_week(week_start, end_time)
        }
        
        return report
```

## ğŸ›¡ï¸ æ—¥å¿—å®‰å…¨

### **1. æ•æ„Ÿä¿¡æ¯è¿‡æ»¤**

```python
class LogSanitizer:
    def __init__(self, sensitive_fields):
        self.sensitive_fields = sensitive_fields
    
    def sanitize_log_data(self, log_data):
        """æ¸…ç†æ—¥å¿—æ•°æ®ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        sanitized_data = log_data.copy()
        
        for field in self.sensitive_fields:
            if field in sanitized_data:
                sanitized_data[field] = '***REDACTED***'
        
        return sanitized_data
    
    def mask_password(self, text):
        """æ©ç å¯†ç """
        import re
        return re.sub(r'password["\']?\s*[:=]\s*["\']?[^"\s]+["\']?', 
                     'password="***REDACTED***"', text, flags=re.IGNORECASE)
```

### **2. æ—¥å¿—åŠ å¯†**

```python
class LogEncryption:
    def __init__(self, encryption_key):
        self.encryption_key = encryption_key
    
    def encrypt_log(self, log_data):
        """åŠ å¯†æ—¥å¿—æ•°æ®"""
        from cryptography.fernet import Fernet
        
        cipher = Fernet(self.encryption_key)
        encrypted_data = cipher.encrypt(json.dumps(log_data).encode())
        return encrypted_data
    
    def decrypt_log(self, encrypted_data):
        """è§£å¯†æ—¥å¿—æ•°æ®"""
        from cryptography.fernet import Fernet
        
        cipher = Fernet(self.encryption_key)
        decrypted_data = cipher.decrypt(encrypted_data)
        return json.loads(decrypted_data.decode())
```

## ğŸ“ˆ æ—¥å¿—æ€§èƒ½ä¼˜åŒ–

### **1. æ—¥å¿—è½®è½¬**

```python
class LogRotator:
    def __init__(self, config):
        self.config = config
    
    def rotate_logs(self):
        """è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        for log_type in ['access', 'audit', 'error', 'application']:
            log_dir = f"logs/{log_type}"
            self.rotate_log_directory(log_dir)
    
    def rotate_log_directory(self, log_dir):
        """è½®è½¬æŒ‡å®šç›®å½•çš„æ—¥å¿—"""
        import os
        import shutil
        
        for filename in os.listdir(log_dir):
            if filename.endswith('.log'):
                file_path = os.path.join(log_dir, filename)
                file_size = os.path.getsize(file_path)
                
                if file_size > self.config['max_file_size']:
                    # åˆ›å»ºå¤‡ä»½æ–‡ä»¶
                    backup_name = f"{filename}.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    backup_path = os.path.join(log_dir, backup_name)
                    shutil.move(file_path, backup_path)
                    
                    # å‹ç¼©å¤‡ä»½æ–‡ä»¶
                    self.compress_file(backup_path)
```

### **2. æ—¥å¿—å‹ç¼©**

```python
class LogCompressor:
    def compress_file(self, file_path):
        """å‹ç¼©æ–‡ä»¶"""
        import gzip
        
        with open(file_path, 'rb') as f_in:
            with gzip.open(f"{file_path}.gz", 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        # åˆ é™¤åŸæ–‡ä»¶
        os.remove(file_path)
    
    def decompress_file(self, compressed_path):
        """è§£å‹æ–‡ä»¶"""
        import gzip
        
        with gzip.open(compressed_path, 'rb') as f_in:
            with open(compressed_path[:-3], 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
```

---

**æ€»ç»“**: Logsæ¨¡å—æä¾›äº†å®Œæ•´çš„æ—¥å¿—ç®¡ç†è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ—¥å¿—æ”¶é›†ã€å­˜å‚¨ã€åˆ†æã€ç›‘æ§å’Œå¯è§†åŒ–åŠŸèƒ½ã€‚é€šè¿‡ç»“æ„åŒ–çš„æ—¥å¿—è®°å½•å’Œå¼ºå¤§çš„åˆ†æå·¥å…·ï¼Œç¡®ä¿ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§å’Œé—®é¢˜æ’æŸ¥èƒ½åŠ›ï¼Œä¸ºç³»ç»Ÿè¿ç»´å’Œæ€§èƒ½ä¼˜åŒ–æä¾›é‡è¦æ”¯æŒã€‚
