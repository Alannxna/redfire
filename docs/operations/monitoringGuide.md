# ğŸ“Š RedFireç›‘æ§è¿ç»´æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»RedFireç³»ç»Ÿçš„ç›‘æ§ä½“ç³»ã€è¿ç»´æµç¨‹å’Œæ•…éšœå¤„ç†ï¼Œå¸®åŠ©è¿ç»´äººå‘˜æœ‰æ•ˆç®¡ç†ç³»ç»Ÿçš„å¥åº·çŠ¶æ€ã€‚

## ğŸ” ç›‘æ§ä½“ç³»

### 1. ç³»ç»Ÿç›‘æ§
- **CPUä½¿ç”¨ç‡**: å®æ—¶ç›‘æ§CPUè´Ÿè½½
- **å†…å­˜ä½¿ç”¨ç‡**: ç›‘æ§å†…å­˜å ç”¨æƒ…å†µ
- **ç£ç›˜I/O**: ç›‘æ§ç£ç›˜è¯»å†™æ€§èƒ½
- **ç½‘ç»œæµé‡**: ç›‘æ§ç½‘ç»œå¸¦å®½ä½¿ç”¨

### 2. åº”ç”¨ç›‘æ§
- **æœåŠ¡çŠ¶æ€**: ç›‘æ§å„æœåŠ¡è¿è¡ŒçŠ¶æ€
- **å“åº”æ—¶é—´**: ç›‘æ§APIæ¥å£å“åº”æ—¶é—´
- **é”™è¯¯ç‡**: ç›‘æ§ç³»ç»Ÿé”™è¯¯å’Œå¼‚å¸¸
- **ååé‡**: ç›‘æ§ç³»ç»Ÿå¤„ç†èƒ½åŠ›

### 3. ä¸šåŠ¡ç›‘æ§
- **äº¤æ˜“é‡**: ç›‘æ§äº¤æ˜“è®¢å•æ•°é‡
- **æˆåŠŸç‡**: ç›‘æ§äº¤æ˜“æˆåŠŸç‡
- **é£é™©æŒ‡æ ‡**: ç›‘æ§é£é™©æ§åˆ¶æŒ‡æ ‡
- **ç”¨æˆ·æ´»è·ƒåº¦**: ç›‘æ§ç”¨æˆ·ä½¿ç”¨æƒ…å†µ

## ğŸ› ï¸ ç›‘æ§å·¥å…·

### Prometheus + Grafana
```yaml
# prometheus.yml é…ç½®
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'redfire-backend'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
    
  - job_name: 'redfire-frontend'
    static_configs:
      - targets: ['localhost:3000']
    metrics_path: '/metrics'
```

### ELK Stack
```yaml
# logstash.conf é…ç½®
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "redfire-backend" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "redfire-logs-%{+YYYY.MM.dd}"
  }
}
```

### è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡
```python
# ç›‘æ§æŒ‡æ ‡å®šä¹‰
from prometheus_client import Counter, Histogram, Gauge

# äº¤æ˜“è®¢å•è®¡æ•°å™¨
ORDER_COUNTER = Counter('redfire_orders_total', 'Total number of orders', ['status', 'symbol'])

# APIå“åº”æ—¶é—´ç›´æ–¹å›¾
API_RESPONSE_TIME = Histogram('redfire_api_response_time_seconds', 'API response time', ['endpoint'])

# ç³»ç»ŸçŠ¶æ€æŒ‡æ ‡
SYSTEM_STATUS = Gauge('redfire_system_status', 'System status indicator')

# è®°å½•è®¢å•
def record_order(status: str, symbol: str):
    ORDER_COUNTER.labels(status=status, symbol=symbol).inc()

# è®°å½•APIå“åº”æ—¶é—´
def record_api_time(endpoint: str, duration: float):
    API_RESPONSE_TIME.labels(endpoint=endpoint).observe(duration)
```

## ğŸ“Š å‘Šè­¦é…ç½®

### å‘Šè­¦è§„åˆ™
```yaml
# alerting.yml é…ç½®
groups:
  - name: redfire-alerts
    rules:
      - alert: HighCPUUsage
        expr: cpu_usage > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "CPUä½¿ç”¨ç‡è¿‡é«˜"
          description: "CPUä½¿ç”¨ç‡è¶…è¿‡80%æŒç»­5åˆ†é’Ÿ"
      
      - alert: HighMemoryUsage
        expr: memory_usage > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡85%æŒç»­5åˆ†é’Ÿ"
      
      - alert: APISlowResponse
        expr: api_response_time > 2
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "APIå“åº”æ—¶é—´è¿‡é•¿"
          description: "APIå“åº”æ—¶é—´è¶…è¿‡2ç§’æŒç»­2åˆ†é’Ÿ"
```

### å‘Šè­¦é€šçŸ¥
```python
# å‘Šè­¦é€šçŸ¥é…ç½®
class AlertNotifier:
    def __init__(self):
        self.webhook_url = os.getenv('ALERT_WEBHOOK_URL')
        self.email_config = self.load_email_config()
    
    def send_webhook(self, alert_data):
        """å‘é€Webhooké€šçŸ¥"""
        try:
            response = requests.post(
                self.webhook_url,
                json=alert_data,
                timeout=10
            )
            response.raise_for_status()
        except Exception as e:
            logger.error(f"Webhooké€šçŸ¥å‘é€å¤±è´¥: {e}")
    
    def send_email(self, alert_data):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        try:
            msg = MIMEText(alert_data['description'])
            msg['Subject'] = f"RedFireå‘Šè­¦: {alert_data['summary']}"
            msg['From'] = self.email_config['from']
            msg['To'] = self.email_config['to']
            
            with smtplib.SMTP(self.email_config['smtp_server']) as server:
                server.login(self.email_config['username'], self.email_config['password'])
                server.send_message(msg)
        except Exception as e:
            logger.error(f"é‚®ä»¶é€šçŸ¥å‘é€å¤±è´¥: {e}")
```

## ğŸ”§ è¿ç»´è„šæœ¬

### å¥åº·æ£€æŸ¥è„šæœ¬
```python
#!/usr/bin/env python3
"""ç³»ç»Ÿå¥åº·æ£€æŸ¥è„šæœ¬"""

import requests
import psutil
import redis
import psycopg2
import logging
from datetime import datetime

class HealthChecker:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.health_status = {}
    
    def check_system_health(self):
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        self.check_cpu_usage()
        self.check_memory_usage()
        self.check_disk_usage()
        self.check_network_status()
        
        return self.health_status
    
    def check_cpu_usage(self):
        """æ£€æŸ¥CPUä½¿ç”¨ç‡"""
        cpu_percent = psutil.cpu_percent(interval=1)
        self.health_status['cpu_usage'] = cpu_percent
        
        if cpu_percent > 80:
            self.logger.warning(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent}%")
    
    def check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡"""
        memory = psutil.virtual_memory()
        self.health_status['memory_usage'] = memory.percent
        
        if memory.percent > 85:
            self.logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent}%")
    
    def check_disk_usage(self):
        """æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡"""
        disk = psutil.disk_usage('/')
        self.health_status['disk_usage'] = disk.percent
        
        if disk.percent > 90:
            self.logger.warning(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {disk.percent}%")
    
    def check_network_status(self):
        """æ£€æŸ¥ç½‘ç»œçŠ¶æ€"""
        try:
            # æ£€æŸ¥ç½‘ç»œè¿æ¥
            response = requests.get('https://www.google.com', timeout=5)
            self.health_status['network_status'] = 'connected'
        except Exception as e:
            self.health_status['network_status'] = 'disconnected'
            self.logger.error(f"ç½‘ç»œè¿æ¥å¤±è´¥: {e}")

if __name__ == "__main__":
    checker = HealthChecker()
    status = checker.check_system_health()
    print(f"å¥åº·æ£€æŸ¥ç»“æœ: {status}")
```

### æ—¥å¿—åˆ†æè„šæœ¬
```python
#!/usr/bin/env python3
"""æ—¥å¿—åˆ†æè„šæœ¬"""

import re
import json
from collections import Counter
from datetime import datetime, timedelta

class LogAnalyzer:
    def __init__(self, log_file_path):
        self.log_file_path = log_file_path
        self.error_patterns = {
            'database_error': r'DatabaseError|ConnectionError',
            'api_error': r'APIError|HTTPError',
            'trading_error': r'TradingError|OrderError',
            'system_error': r'SystemError|RuntimeError'
        }
    
    def analyze_errors(self, hours=24):
        """åˆ†ææŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„é”™è¯¯"""
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        error_counts = Counter()
        error_details = []
        
        with open(self.log_file_path, 'r') as f:
            for line in f:
                try:
                    # è§£ææ—¥å¿—æ—¶é—´
                    time_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                    if time_match:
                        log_time = datetime.strptime(time_match.group(1), '%Y-%m-%d %H:%M:%S')
                        
                        if start_time <= log_time <= end_time:
                            # æ£€æŸ¥é”™è¯¯ç±»å‹
                            for error_type, pattern in self.error_patterns.items():
                                if re.search(pattern, line, re.IGNORECASE):
                                    error_counts[error_type] += 1
                                    error_details.append({
                                        'time': log_time.isoformat(),
                                        'type': error_type,
                                        'message': line.strip()
                                    })
                except Exception as e:
                    continue
        
        return {
            'error_counts': dict(error_counts),
            'error_details': error_details,
            'total_errors': sum(error_counts.values())
        }
    
    def generate_report(self, hours=24):
        """ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Š"""
        analysis = self.analyze_errors(hours)
        
        report = f"""
RedFireç³»ç»Ÿé”™è¯¯åˆ†ææŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†æèŒƒå›´: æœ€è¿‘{hours}å°æ—¶
æ€»é”™è¯¯æ•°: {analysis['total_errors']}

é”™è¯¯ç±»å‹ç»Ÿè®¡:
"""
        
        for error_type, count in analysis['error_counts'].items():
            report += f"- {error_type}: {count}æ¬¡\n"
        
        if analysis['error_details']:
            report += "\nè¯¦ç»†é”™è¯¯ä¿¡æ¯:\n"
            for error in analysis['error_details'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                report += f"- {error['time']} [{error['type']}]: {error['message']}\n"
        
        return report

if __name__ == "__main__":
    analyzer = LogAnalyzer('/var/log/redfire/app.log')
    report = analyzer.generate_report(24)
    print(report)
```

## ğŸš¨ æ•…éšœå¤„ç†

### å¸¸è§æ•…éšœåŠè§£å†³æ–¹æ¡ˆ

#### 1. æœåŠ¡æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
systemctl status redfire-backend

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
journalctl -u redfire-backend -f

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep :8000

# é‡å¯æœåŠ¡
systemctl restart redfire-backend
```

#### 2. æ•°æ®åº“è¿æ¥å¤±è´¥
```bash
# æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€
systemctl status postgresql

# æµ‹è¯•æ•°æ®åº“è¿æ¥
psql -h localhost -U redfire -d redfire -c "SELECT 1;"

# æ£€æŸ¥æ•°æ®åº“æ—¥å¿—
tail -f /var/log/postgresql/postgresql-*.log

# é‡å¯æ•°æ®åº“æœåŠ¡
systemctl restart postgresql
```

#### 3. å†…å­˜ä¸è¶³
```bash
# æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
free -h

# æŸ¥çœ‹å†…å­˜å ç”¨è¿›ç¨‹
ps aux --sort=-%mem | head -10

# æ¸…ç†ç¼“å­˜
echo 3 > /proc/sys/vm/drop_caches

# é‡å¯é«˜å†…å­˜å ç”¨æœåŠ¡
systemctl restart redis
```

### æ•…éšœå‡çº§æµç¨‹
1. **ä¸€çº§æ•…éšœ**: è¿ç»´äººå‘˜å¤„ç†
2. **äºŒçº§æ•…éšœ**: å¼€å‘å›¢é˜ŸååŠ©
3. **ä¸‰çº§æ•…éšœ**: æ¶æ„å¸ˆä»‹å…¥
4. **ç´§æ€¥æ•…éšœ**: ç«‹å³é€šçŸ¥ç®¡ç†å±‚

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### ç³»ç»Ÿä¼˜åŒ–
```bash
# ä¼˜åŒ–å†…æ ¸å‚æ•°
echo 'vm.swappiness=10' >> /etc/sysctl.conf
echo 'net.core.somaxconn=65535' >> /etc/sysctl.conf
sysctl -p

# ä¼˜åŒ–æ–‡ä»¶ç³»ç»Ÿ
echo 'noatime,nodiratime' >> /etc/fstab
mount -o remount /

# ä¼˜åŒ–ç½‘ç»œå‚æ•°
echo 'net.ipv4.tcp_fin_timeout=30' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_keepalive_time=1200' >> /etc/sysctl.conf
```

### åº”ç”¨ä¼˜åŒ–
```python
# æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
DATABASE_CONFIG = {
    'min_size': 5,
    'max_size': 20,
    'max_queries': 50000,
    'max_inactive_connection_lifetime': 300.0
}

# Redisè¿æ¥æ± ä¼˜åŒ–
REDIS_CONFIG = {
    'max_connections': 20,
    'retry_on_timeout': True,
    'socket_keepalive': True
}

# å¼‚æ­¥ä»»åŠ¡ä¼˜åŒ–
CELERY_CONFIG = {
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_max_tasks_per_child': 1000
}
```

## ğŸ“‹ è¿ç»´æ£€æŸ¥æ¸…å•

### æ—¥å¸¸æ£€æŸ¥
- [ ] ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
- [ ] æœåŠ¡è¿è¡ŒçŠ¶æ€
- [ ] æ•°æ®åº“è¿æ¥çŠ¶æ€
- [ ] æ—¥å¿—æ–‡ä»¶å¤§å°
- [ ] ç£ç›˜ç©ºé—´ä½¿ç”¨

### å‘¨æœŸæ€§æ£€æŸ¥
- [ ] æ€§èƒ½æŒ‡æ ‡åˆ†æ
- [ ] é”™è¯¯æ—¥å¿—åˆ†æ
- [ ] å®‰å…¨æ¼æ´æ‰«æ
- [ ] å¤‡ä»½çŠ¶æ€æ£€æŸ¥
- [ ] ç›‘æ§å‘Šè­¦æµ‹è¯•

### åº”æ€¥å“åº”
- [ ] æ•…éšœå¿«é€Ÿå®šä½
- [ ] æœåŠ¡å¿«é€Ÿæ¢å¤
- [ ] å½±å“èŒƒå›´è¯„ä¼°
- [ ] ç”¨æˆ·é€šçŸ¥å‘å¸ƒ
- [ ] äº‹ååˆ†ææŠ¥å‘Š

---

*RedFireç›‘æ§è¿ç»´æŒ‡å— - ä¿éšœç³»ç»Ÿç¨³å®šè¿è¡Œï¼Œæå‡è¿ç»´æ•ˆç‡* ğŸ”¥
